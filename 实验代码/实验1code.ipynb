{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff691c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell  #执行该代码可以使得当前nb支持多输出\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "pd.options.display.max_rows = 8  \n",
    "import requests\n",
    "import parsel\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from time import sleep\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome,ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "\n",
    "#忽略警告\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681dca04",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee324500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理年份 2013 的链接\n",
      "已处理年份 2014 的链接\n",
      "已处理年份 2015 的链接\n",
      "已处理年份 2016 的链接\n",
      "已处理年份 2017 的链接\n",
      "已处理年份 2018 的链接\n",
      "已处理年份 2019 的链接\n",
      "已处理年份 2020 的链接\n",
      "已处理年份 2021 的链接\n",
      "已处理年份 2022 的链接\n",
      "已处理年份 2023 的链接\n",
      "链接字典已保存到 ./实验一爬取数据/link_dict.json\n"
     ]
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 设置请求头\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 定义要提取的年份范围\n",
    "years = range(2013, 2024)\n",
    "\n",
    "# 初始化链接字典\n",
    "link_dict = {}\n",
    "\n",
    "# 确保实验一爬取数据文件夹存在\n",
    "os.makedirs(\"./实验一爬取数据\", exist_ok=True)\n",
    "\n",
    "# 循环处理每个年份\n",
    "for year in years:\n",
    "    # 构造年份对应的索引页面URL\n",
    "    index_url = f\"https://www.cnipa.gov.cn/tjxx/jianbao/year{year}/indexy.html\"\n",
    "    \n",
    "    try:\n",
    "        # 发送HTTP请求获取索引页面内容\n",
    "        response = requests.get(index_url, headers=headers)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"获取索引页面 {index_url} 失败: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 解析索引页面，寻找包含特定关键字的链接\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # 筛选包含特定关键字的链接\n",
    "    target_keywords = ['专利申请代理状况', '专利申请、授权按IPC分类分布状况', '商标申请、注册及有效注册状况']\n",
    "    target_link_pages = []\n",
    "    for link in links:\n",
    "        # 检查链接文本是否包含目标关键词之一\n",
    "        if any(keyword in link.text for keyword in target_keywords):\n",
    "            href = link['href']\n",
    "            # 构造完整的链接URL\n",
    "            full_url = urljoin(index_url, href)\n",
    "            target_link_pages.append(full_url)\n",
    "    \n",
    "    # 对每个目标链接页面进行进一步处理\n",
    "    link_dict[year] = []  # 初始化当年份的列表\n",
    "    for target_link_page in target_link_pages:\n",
    "        try:\n",
    "            # 发送HTTP请求获取目标链接页面内容\n",
    "            target_response = requests.get(target_link_page, headers=headers)\n",
    "            target_response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"获取目标链接页面 {target_link_page} 失败: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # 解析目标链接页面，提取其中的所有链接\n",
    "        target_soup = BeautifulSoup(target_response.text, 'html.parser')\n",
    "        sub_links = target_soup.find_all('a', href=True)\n",
    "        \n",
    "        # 过滤并整理子链接，排除包含 'indexy' 的链接\n",
    "        sub_target_links = []\n",
    "        for sub_link in sub_links:\n",
    "            sub_href = sub_link['href']\n",
    "            # 过滤掉非HTML文件的链接以及包含 'indexy' 的链接\n",
    "            if sub_href.endswith('.html') and 'indexy' not in sub_href:\n",
    "                # 构造完整的子链接URL\n",
    "                sub_full_url = urljoin(target_link_page, sub_href)\n",
    "                sub_target_links.append(sub_full_url)\n",
    "        \n",
    "        # 假设目标链接页面的标题作为大类名称\n",
    "        # 尝试从目标链接页面中提取大类名称\n",
    "        class_name = target_soup.find('title')\n",
    "        if class_name:\n",
    "            class_name = class_name.text.strip()\n",
    "        else:\n",
    "            # 如果没有标题，使用链接中的文件名作为大类名称\n",
    "            class_name = os.path.basename(target_link_page).split('.')[0]\n",
    "        \n",
    "        # 将大类名称和子链接数组存储到链接字典中\n",
    "        link_dict[year].append({\n",
    "            'class_name': class_name,\n",
    "            'links': sub_target_links\n",
    "        })\n",
    "    \n",
    "    print(f\"已处理年份 {year} 的链接\")\n",
    "\n",
    "# 保存链接字典到JSON文件\n",
    "with open('./实验一爬取数据/link_dict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(link_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"链接字典已保存到 ./实验一爬取数据/link_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8640149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process 专利申请代理状况-d1: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请代理状况-d2: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请代理状况-d3: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请代理状况-d4: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e3: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e4: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e5: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e6: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e7: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e8: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 商标申请、注册及有效注册状况-h1: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 商标申请、注册及有效注册状况-h2: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 商标申请、注册及有效注册状况-h3: index 0 is out of bounds for axis 0 with size 0\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h4.csv\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h4.csv\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h4.csv\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g4.csv\n",
      "处理失败的链接已保存到 '实验一爬取数据/failed_links.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 设置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "failed_links = []\n",
    "\n",
    "def crawl_and_process(year, class_name, link, table_title):\n",
    "    \"\"\"\n",
    "    爬取并处理网页表格数据，并保存为CSV文件\n",
    "    \"\"\"\n",
    "    # 创建对应年份的文件夹（如果不存在的话）\n",
    "    folder_path = f'实验一爬取数据/{year}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # 发送 GET 请求获取网页内容\n",
    "    response = requests.get(link, headers=headers)\n",
    "    # 设置正确的编码格式\n",
    "    response.encoding = 'utf-8'\n",
    "    # 使用 BeautifulSoup 解析 HTML 内容\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # 查找所有表格\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # 检查是否存在表格\n",
    "    if not tables:\n",
    "        print(f\"No tables found in {link}\")\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"No tables found\"})\n",
    "        return\n",
    "    \n",
    "    # 尝试获取第二个表格（根据之前的代码逻辑）\n",
    "    specified_tables = tables[1] if len(tables) > 1 else tables[0]\n",
    "    \n",
    "    # 将表格转换为 DataFrame\n",
    "    try:\n",
    "        df = pd.read_html(str(specified_tables))[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse table in {link}: {e}\")\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": str(e)})\n",
    "        return\n",
    "    \n",
    "    # 对表格进行数据处理\n",
    "    if len(df) > 4:\n",
    "        df = df[4:]\n",
    "        # 检查索引是否存在，避免越界错误\n",
    "        indices_to_drop = [idx for idx in [4, 6] if idx in df.index]\n",
    "        df.drop(indices_to_drop, inplace=True)\n",
    "    \n",
    "    # 检查 DataFrame 是否为空\n",
    "    if df.empty:\n",
    "        print(f'No data to save for {class_name}-{table_title}')\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"Empty DataFrame\"})\n",
    "        return\n",
    "    \n",
    "    # 检查列数是否一致\n",
    "    if len(df.columns) < 1:\n",
    "        print(f'No columns found in DataFrame for {class_name}-{table_title}')\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"No columns found\"})\n",
    "        return\n",
    "    \n",
    "    # 尝试进行更复杂的数据处理\n",
    "    try:\n",
    "        # 设置第一列作为索引\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "        df.index.name = None\n",
    "        \n",
    "        # 将第一行作为列名\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        df.columns.name = None\n",
    "        \n",
    "        # 将原有索引设置为一级索引，同时将第一列和第一行作为二级索引\n",
    "        df = df.set_index(df.columns[0], append=True)\n",
    "        df.index.name = None  # 删除索引列的名称\n",
    "        \n",
    "        # 创建二级索引\n",
    "        df.columns = pd.MultiIndex.from_arrays([df.iloc[0], df.columns], names=[None, None])\n",
    "        df = df.iloc[1:]  # 删除原始的第一行\n",
    "        df = df.swaplevel(axis=1)\n",
    "        \n",
    "        # 保存为 CSV 文件，如果 DataFrame 不为空\n",
    "        if not df.empty:\n",
    "            df.to_csv(f'{folder_path}/{class_name}-{table_title}.csv', encoding='utf-8-sig')\n",
    "            print(f'Successfully saved {class_name}-{table_title}.csv')\n",
    "        else:\n",
    "            print(f'No data to save for {class_name}-{table_title}')\n",
    "            failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"Empty DataFrame after processing\"})\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {class_name}-{table_title}: {e}\")\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": str(e)})\n",
    "\n",
    "# 加载 JSON 文件\n",
    "with open(r'实验一爬取数据/link_dict.json', 'r', encoding='utf-8') as f:\n",
    "    link_dict = json.load(f)\n",
    "\n",
    "# 遍历每个年份和类别链接，进行爬取和处理\n",
    "for year in link_dict:\n",
    "    if link_dict[year]:  # 如果该年份的类别列表不为空\n",
    "        for class_info in link_dict[year]:\n",
    "            class_name = class_info[\"class_name\"]\n",
    "            for link in class_info[\"links\"]:\n",
    "                # 从链接中提取表格标题\n",
    "                table_title = link.split('/')[-1].replace('.html', '')\n",
    "                crawl_and_process(year, class_name, link, table_title)\n",
    "\n",
    "# 保存处理失败的链接到 JSON 文件\n",
    "with open(r'实验一爬取数据/failed_links.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(failed_links, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"处理失败的链接已保存到 '实验一爬取数据/failed_links.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b502a",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058621d",
   "metadata": {
    "vscode": {
     "languageId": "groovy"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "url = 'https://www.baiten.cn/results/filter'\n",
    "\n",
    "# Select Zhejiang province\n",
    "province = '浙江'\n",
    "\n",
    "df = pd.DataFrame(dtype=str)\n",
    "\n",
    "for year in range(2000, 2025):\n",
    "    params = {\n",
    "        'q': 'aa:((' + province + ')) AND (pd:[' + str(year) + '0101 TO ' + str(year) + '1231])',\n",
    "        'fq': '',\n",
    "        'sc': '35184372088831'\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Cookie': '_uab_collina=171636084297048745836638; yunsuo_session_verify=651468e84a027b9680fb898cb3261dac; JSESSIONID=467E8A8E43574AD6ADDD2CA1AA8E598B; zlcp2024423=true; BSESSION=e2148644ecde040ae13f44b2ace4184a38f0d6b0623961ad; PD=a6e2323d0e6adfd16b0bb9a0e28457c5d0c7bf66bf1f72327d2d56e14cb82af5a5f1e3e7ae91cf98c7fa13fa30f3d1fd'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(url=url, params=params, headers=headers)\n",
    "        page_json = resp.json()\n",
    "        \n",
    "        # Extract patent counts by type\n",
    "        cn_um = int(page_json['facetPivots']['country']['cn'].get('cn_um', 0))  # Utility model\n",
    "        cn_id = int(page_json['facetPivots']['country']['cn'].get('cn_id', 0))  # Design\n",
    "        cn_in = int(page_json['facetPivots']['country']['cn'].get('cn_in', 0))  # Invention\n",
    "        cn_gp = int(page_json['facetPivots']['country']['cn'].get('cn_gp', 0))  # Granted invention\n",
    "        cn_in_gp = int(page_json['facetPivots']['country']['cn'].get('cn_in_gp', 0))  # Published invention\n",
    "        \n",
    "        # Add to dataframe\n",
    "        df = pd.concat([df, pd.DataFrame({\n",
    "            '发明专利': cn_in,\n",
    "            '发明公开专利': cn_in_gp,\n",
    "            '发明授权专利': cn_gp,\n",
    "            '外观专利': cn_id,\n",
    "            '实用新型专利': cn_um,\n",
    "            '年份': year,\n",
    "            '省份': province\n",
    "        }, index=[len(df)])], axis=0)\n",
    "        \n",
    "        print(f'Successfully collected data for {year}')\n",
    "        sleep(random.randint(1,3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Failed to collect data for {year}: {e}')\n",
    "        continue\n",
    "\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel(f'./实验一爬取数据/佰腾网/佰腾网{province}2000-2024专利数据.xlsx', index=False)\n",
    "print('Data collection completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8cfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  登录微博\n",
    "# 导入需要的库\n",
    "from selenium import webdriver  # Selenium用于模拟浏览器操作\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains  \n",
    "import time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f13ed1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWebDriverException\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/common/driver_finder.py:67\u001b[39m, in \u001b[36mDriverFinder._binary_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     output = \u001b[43mSeleniumManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m Path(output[\u001b[33m\"\u001b[39m\u001b[33mdriver_path\u001b[39m\u001b[33m\"\u001b[39m]).is_file():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/common/selenium_manager.py:55\u001b[39m, in \u001b[36mSeleniumManager.binary_paths\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m     53\u001b[39m args.append(\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/common/selenium_manager.py:129\u001b[39m, in \u001b[36mSeleniumManager._run\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m completed_proc.returncode:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WebDriverException(\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsuccessful command executed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompleted_proc.returncode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m     )\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mWebDriverException\u001b[39m: Message: Unsuccessful command executed: /home/gyh/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/common/linux/selenium-manager --browser chrome --language-binding python --output json; code: 65\n{'code': 65, 'message': 'error sending request for url (https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json)', 'driver_path': '', 'browser_path': ''}\n\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNoSuchDriverException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 启动浏览器\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m browser = \u001b[43mwebdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 启动Chrome浏览器，确保已经安装Chrome浏览器和对应的ChromeDriver驱动程序\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 打开微博首页\u001b[39;00m\n\u001b[32m      5\u001b[39m browser.get(\u001b[33m\"\u001b[39m\u001b[33mhttps://passport.weibo.com/sso/signin?entry=miniblog&source=miniblog&disp=popup&url=https\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3A\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[33mweibo.com\u001b[39m\u001b[38;5;132;01m%2F\u001b[39;00m\u001b[33mnewlogin\u001b[39m\u001b[38;5;132;01m%3F\u001b[39;00m\u001b[33mtabtype\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3Dweibo\u001b[39m\u001b[38;5;132;01m%26g\u001b[39;00m\u001b[33mid\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3D102803\u001b[39m\u001b[38;5;132;01m%26o\u001b[39;00m\u001b[33mpenLoginLayer\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3D0\u001b[39m\u001b[38;5;132;01m%26u\u001b[39;00m\u001b[33mrl\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3Dhttps\u001b[39m\u001b[33m%\u001b[39m\u001b[33m253A\u001b[39m\u001b[38;5;132;01m%252F\u001b[39;00m\u001b[38;5;132;01m%252F\u001b[39;00m\u001b[33mweibo.com\u001b[39m\u001b[38;5;132;01m%252F\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 使用浏览器打开微博首页\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[39m, in \u001b[36mWebDriver.__init__\u001b[39m\u001b[34m(self, options, service, keep_alive)\u001b[39m\n\u001b[32m     42\u001b[39m service = service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[32m     43\u001b[39m options = options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbrowserName\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/chromium/webdriver.py:51\u001b[39m, in \u001b[36mChromiumDriver.__init__\u001b[39m\u001b[34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.service = service\n\u001b[32m     50\u001b[39m finder = DriverFinder(\u001b[38;5;28mself\u001b[39m.service, options)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     52\u001b[39m     options.binary_location = finder.get_browser_path()\n\u001b[32m     53\u001b[39m     options.browser_version = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/common/driver_finder.py:47\u001b[39m, in \u001b[36mDriverFinder.get_browser_path\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mbrowser_path\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/common/driver_finder.py:78\u001b[39m, in \u001b[36mDriverFinder._binary_paths\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     77\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paths\n",
      "\u001b[31mNoSuchDriverException\u001b[39m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "# # 启动浏览器\n",
    "# browser = webdriver.Chrome()  # 启动Chrome浏览器，确保已经安装Chrome浏览器和对应的ChromeDriver驱动程序\n",
    "\n",
    "# # 打开微博首页\n",
    "# browser.get(\"https://passport.weibo.com/sso/signin?entry=miniblog&source=miniblog&disp=popup&url=https%3A%2F%2Fweibo.com%2Fnewlogin%3Ftabtype%3Dweibo%26gid%3D102803%26openLoginLayer%3D0%26url%3Dhttps%253A%252F%252Fweibo.com%252F\")  # 使用浏览器打开微博首页\n",
    "\n",
    "# # 等待页面加载完成\n",
    "# time.sleep(3)  \n",
    "\n",
    "# # 定位要点击的元素\n",
    "# button = browser.find_element(By.XPATH, \"//*[@id='app']/div/div/div[2]/div[2]/ul/li[2]/a/span[2]\")  # 用XPath定位元素，可以根据实际情况修改 //*[@id=\"app\"]/div/div/div[2]/div/ul/li[2]/a/span[1]\n",
    "# # 执行点击操作\n",
    "# button.click()\n",
    "\n",
    "# # 定位账号输入框，并填入账号\n",
    "# user_input = browser.find_element(By.XPATH,\"//*[@id='app']/div/div/div[2]/div[2]/form/div[1]/input\")  # 使用id属性定位账号输入框\n",
    "# user_input.send_keys(\"17763769712\")  # 向账号输入框输入你的用户名\n",
    "\n",
    "# # 定位密码输入框，并填入密码\n",
    "# pwd_input = browser.find_element(By.XPATH,\"//*[@id='app']/div/div/div[2]/div[2]/form/div[2]/input\")  # 使用name属性定位密码输入框\n",
    "# pwd_input.send_keys(\"Yyzz20030618\")  # 向密码输入框输入你的密码\n",
    "\n",
    "# # 等待登录完成\n",
    "# time.sleep(3)  # 等待10秒钟，确保登录成功和页面跳转完成，这里的时间可以根据实际情况进行调整\n",
    "\n",
    "# # 定位要点击的是登录按钮\n",
    "# button = browser.find_element(By.XPATH,\"//*[@id='app']/div/div/div[2]/div[2]/button\")\n",
    "# # 点击登录按钮\n",
    "# button.click()\n",
    "\n",
    "# # # 滚动条拉到指定位置（具体元素）  \n",
    "# # target = browser.find_element(By.XPATH,\"//*[@class='slider']\")  \n",
    "# # browser.execute_script(\"arguments[0].scrollIntoView();\", target)  \n",
    "\n",
    "# time.sleep(3)  \n",
    "# cookies = browser.get_cookies()\n",
    "# print(cookies)\n",
    "# # browser.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "######北京知识产权官方微博数据抓取\n",
    "import requests\n",
    "import csv\n",
    "from lxml import etree\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce97d95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_full_content(page):\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.51',\n",
    "        'cookie': '_T_WM=29007828263; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFPrQQDSM8GU-LDwXGJfO-75JpX5K-hUgL.FoMRSh-f1hqNSoq2dJLoI7p0dNHjdg8X9HvodGnt; SSOLoginState=1687401343; ALF=1689993343; SCF=AliNZPJfd8jrC1kjeXX5Wln4GfwuiZk5G2brx6c1VvQ_NS49PAH7r8V1CrDXL_YlLEmhT_4y5HWR4ywbGEllBaM.; SUB=_2A25Jl8MvDeRhGeFG71cU-CjLzTqIHXVre-1nrDV6PUJbktANLXT_kW1NeWK98E_aztGw8jkuazsPlMRFEQEXXDZy'\n",
    "    }\n",
    "    content = page.xpath('./div/span[@class=\"ctt\"]//text()')\n",
    "\n",
    "    full_content = ''\n",
    "    for i in range(len(content)):\n",
    "        full_content += content[i]\n",
    "\n",
    "    if u'全文' in full_content:\n",
    "        full_content_link = page.xpath('./div/span[@class=\"ctt\"]/a/@href')\n",
    "        content_url = 'https://weibo.cn' + full_content_link[-1]\n",
    "        response = requests.get(url=content_url, headers=headers)\n",
    "        tree = etree.HTML(response.text.encode('utf-8'))\n",
    "        div = tree.xpath('//*[@id=\"M_\"]/div[1]/span[@class=\"ctt\"]//text()')\n",
    "        full_content = ''\n",
    "        for j in range(len(div)):\n",
    "            full_content += div[j]\n",
    "        return full_content\n",
    "    else:\n",
    "        return full_content\n",
    "\n",
    "\n",
    "def format_time(time_info):\n",
    "    now = datetime.now()\n",
    "    if '今天' in time_info:\n",
    "        time_str = time_info.replace('今天', now.strftime('%Y-%m-%d'))\n",
    "    elif '月' in time_info and '日' in time_info:\n",
    "        month = time_info.split('月')[0]\n",
    "        day = time_info.split('月')[1].split('日')[0]\n",
    "        time_str = now.strftime('%Y') + '-' + month.zfill(2) + '-' + day.zfill(2)\n",
    "    else:\n",
    "        time_str = time_info\n",
    "    return time_str\n",
    "\n",
    "\n",
    "def extract_info(page):\n",
    "    content = extract_full_content(page)\n",
    "    time_info = page.xpath('.//span[@class=\"ct\"]/text()')[0].split('\\xa0')[0]\n",
    "    time_str = format_time(time_info)\n",
    "    source = page.xpath('.//span[@class=\"ct\"]/text()')[0].split('\\xa0')[1][2:]\n",
    "    like_count = page.xpath('.//a[contains(text(), \"赞[\")]/text()')[0][2:-1]\n",
    "    comment_count = page.xpath('.//a[contains(text(), \"评论[\")]/text()')[0][3:-1]\n",
    "    repost_count = page.xpath('.//a[contains(text(), \"转发[\")]/text()')[0][3:-1]\n",
    "    return content, time_str, source, like_count, comment_count, repost_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ced684",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://weibo.cn/2611704875?mp=1302&page='\n",
    "base_url = 'https://weibo.cn/2611704875?mp=1302&page='\n",
    "\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36 Edg/125.0.0.0',\n",
    "    'cookie': '_T_WM=8f1c0a26d8ada0f4491205a111d59473; SCF=Alzst0ONGEqytA_Ri0CzQj0KKN9QRFV9lcOiz2ZJ7xKiYrZnDRpnPqWdtgbyG8KAn0wfqZ9VtQPZk8EAnEHRy7I.; SUB=_2A25LSxo6DeRhGeFH6lsZ9yvIyD-IHXVoKRPyrDV6PUJbktANLWjwkW1Ne9dAHCZXYumFfl_qjyxmUhO7cAm8qCoA; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWJLVu-LI4uxDCyKD4XGJqV5JpX5KMhUgL.FoM4eK.RS0-Xe0e2dJLoIp7LxKML1KBLBKnLxKqL1hnLBoMfeo2NSh.ESh-X; SSOLoginState=1716480618; ALF=1719072618'\n",
    "}\n",
    "\n",
    "filename = '北京知识产权微博内容.csv'\n",
    "\n",
    "# Delete the existing file if it exists\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "    print('Existing data.csv file deleted.')\n",
    "\n",
    "with open(filename, mode='a', encoding='utf-8', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        time.sleep(random.randint(1, 8))\n",
    "        current_url = base_url + str(page_number)+\"&rand=\"+str(random.randint(1, 10000))+\"&p=r\"\n",
    "        try:\n",
    "            response = requests.get(url=current_url, headers=headers)\n",
    "        except ConnectionError:\n",
    "            print('连接失败，跳过该页:', current_url)\n",
    "            continue\n",
    "        tree = etree.HTML(response.text.encode('utf-8'))\n",
    "        data_list = tree.xpath('/html/body/div[@class=\"c\"]')\n",
    "        if not data_list:\n",
    "            break  # No more pages, exit the loop\n",
    "        del data_list[-1]\n",
    "        print('正在爬取第{}页'.format(page_number))\n",
    "        for data in data_list:\n",
    "            content, time_info, source, like_count, comment_count, repost_count = extract_info(data)\n",
    "            csv_writer.writerow([content, time_info, source, like_count, comment_count, repost_count])\n",
    "        page_number += 1\n",
    "\n",
    "print('Extraction completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ed4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('北京知识产权微博内容.csv')\n",
    "# 创建新的列名列表\n",
    "new_columns = ['微博内容', '时间', '来源', '点赞数', '评论数', '转发数']\n",
    "\n",
    "# 将列名作为内容添加到新的行\n",
    "df.loc[-1] = df.columns\n",
    "df.index = df.index + 1\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# 重命名列名\n",
    "df.columns = new_columns\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6540c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('北京知识产权微博内容.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00200857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell  #执行该代码可以使得当前nb支持多输出\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cc681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#这个爬取没问题，但是会有连接时间问题，建议选时间跨度不要太大\n",
    "province_text = '广东、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏、新疆、北京、天津、上海、重庆、香港、澳门'\n",
    "all_province_list = list(province_text.split('、'))\n",
    "df = pd.DataFrame(dtype=str)\n",
    "\n",
    "for province in all_province_list:\n",
    "    for i in range(2020,2025):\n",
    "        url = 'https://www.baiten.cn/results/filter'\n",
    "        params = {\n",
    "            'q': 'aa:((' + province + ')) AND (pd:[' + str(i) + '0101 TO ' + str(i) + '1231])',\n",
    "            'fq': '',\n",
    "            'sc': '35184372088831'\n",
    "        }\n",
    "        headers = {\n",
    "            'Cookie': '_uab_collina=171636084297048745836638; yunsuo_session_verify=651468e84a027b9680fb898cb3261dac; JSESSIONID=467E8A8E43574AD6ADDD2CA1AA8E598B; zlcp2024423=true; BSESSION=e2148644ecde040ae13f44b2ace4184a38f0d6b0623961ad; PD=a6e2323d0e6adfd16b0bb9a0e28457c5d0c7bf66bf1f72327d2d56e14cb82af5a5f1e3e7ae91cf98c7fa13fa30f3d1fd'\n",
    "        }\n",
    "        resp = requests.post(url=url,params=params,headers=headers)\n",
    "        page_json = resp.json()\n",
    "        resp.close()\n",
    "        \n",
    "        cn_um,cn_id,cn_in,cn_gp,cn_in_gp = 0,0,0,0,0\n",
    "        try:    \n",
    "            cn_um = int(page_json['facetPivots']['country']['cn']['cn_um'])\n",
    "        except:\n",
    "            print(province,i,'实用新型专利')\n",
    "        try:\n",
    "            cn_id = int(page_json['facetPivots']['country']['cn']['cn_id'])\n",
    "        except:\n",
    "            print(province,i,'外观专利')\n",
    "        try:\n",
    "            cn_in = int(page_json['facetPivots']['country']['cn']['cn_in'])\n",
    "        except:\n",
    "            print(province,i,'发明专利')\n",
    "        try:\n",
    "            cn_gp = int(page_json['facetPivots']['country']['cn']['cn_gp'])\n",
    "        except:\n",
    "            print(province,i,'发明授权专利')\n",
    "        try:\n",
    "            cn_in_gp = int(page_json['facetPivots']['country']['cn']['cn_in_gp'])\n",
    "        except:\n",
    "            print(province,i,'发明公开专利')\n",
    "        \n",
    "        df = pd.concat([df,pd.DataFrame({'发明专利':cn_in,'发明公开专利':cn_in_gp,'发明授权专利':cn_gp,\n",
    "                                         '外观专利':cn_id,'实用新型专利':cn_um,'年份':i,'省份':province},\n",
    "                                       index=[len(df)])],axis=0)\n",
    "    sleep(random.randint(1,3))\n",
    "    \n",
    "df\n",
    "df.to_excel('./实验一爬取数据/佰腾网/佰腾网2020-2025全国专利数据.xlsx')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdfcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38dcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99433343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179ef5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = \"20250101\"\n",
    "end_date = \"20250102\"\n",
    "\n",
    "# 创建一个空的DataFrame用于存储数据\n",
    "df = pd.DataFrame(columns=['网址', '标题', '时间'])\n",
    "\n",
    "# 设置请求头(headers)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.51',\n",
    "    'Cookie': 'Hm_lvt_162a2ae8b540393bd5792dda2692396e=1687362784; PHPSESSID=214cd4ed0774bc1bd19001d1640ebe75; uuid=CgI1QGSTHOEAMh3oECWbAg==; Hm_lpvt_162a2ae8b540393bd5792dda2692396e=1687447774'\n",
    "}\n",
    "\n",
    "# 循环遍历日期范围\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    date = pd.to_datetime(current_date, format=\"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    print('\\n')\n",
    "    print(\"===================\" + date + \"===================\")\n",
    "    url = f\"https://bjrbdzb.bjd.com.cn/bjrb/mobile/2025/{current_date}/{current_date}_m.html#page0\"\n",
    "    response = requests.get(url, headers=headers, cookies=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 提取网址和标题\n",
    "    links = soup.find_all('a', attrs={'data-href': True})\n",
    "    for index, link in enumerate(links, start=1):\n",
    "        mid_l = len(df.index)\n",
    "        href = link['data-href']\n",
    "        href = \"https://bjrbdzb.bjd.com.cn/bjrb/mobile/2025/\" + current_date + \"/\" + href[2:]  # 更新网址格式\n",
    "        title = link.text.strip()\n",
    "\n",
    "        # 将数据添加到DataFrame中\n",
    "#         df = df.append({'网址': href, '标题': title, '时间': current_date}, ignore_index=True)\n",
    "        df.loc[mid_l] = [href,title,current_date]\n",
    "\n",
    "\n",
    "        response = requests.get(href, headers=headers, cookies=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content_b = soup.find_all('b')\n",
    "        b_texts = [tag.get_text() for tag in content_b]\n",
    "        content_p = soup.find_all('p')\n",
    "        p_texts = [tag.get_text() for tag in content_p]\n",
    "\n",
    "\n",
    "        with open(\"./实验一爬取数据/北京日报/\"+b_texts[0]+\".txt\",'w') as f:\n",
    "            f.write(b_texts[0]+\"\\n\")\n",
    "            for i in p_texts:\n",
    "                f.write(i+\"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "        # 打印保存进度\n",
    "        print(f\"已将{date}第{index}篇文章中{title}写入北京日报文件夹\")\n",
    "\n",
    "    # 更新日期\n",
    "    current_date = pd.to_datetime(current_date, format=\"%Y%m%d\")\n",
    "    current_date += pd.DateOffset(days=1)\n",
    "    current_date = current_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "# 将DataFrame保存到Excel文件\n",
    "df.to_excel(\"./实验一爬取数据/Python实验一省机关报数据的抓取（以北京日报为例）.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c3668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
