{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff691c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell  #执行该代码可以使得当前nb支持多输出\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "pd.options.display.max_rows = 8  \n",
    "import requests\n",
    "import parsel\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from time import sleep\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome,ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "\n",
    "#忽略警告\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681dca04",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee324500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理年份 2013 的链接\n",
      "已处理年份 2014 的链接\n",
      "已处理年份 2015 的链接\n",
      "已处理年份 2016 的链接\n",
      "已处理年份 2017 的链接\n",
      "已处理年份 2018 的链接\n",
      "已处理年份 2019 的链接\n",
      "已处理年份 2020 的链接\n",
      "已处理年份 2021 的链接\n",
      "已处理年份 2022 的链接\n",
      "已处理年份 2023 的链接\n",
      "链接字典已保存到 ./实验一爬取数据/link_dict.json\n"
     ]
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 设置请求头\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 定义要提取的年份范围\n",
    "years = range(2013, 2024)\n",
    "\n",
    "# 初始化链接字典\n",
    "link_dict = {}\n",
    "\n",
    "# 确保实验一爬取数据文件夹存在\n",
    "os.makedirs(\"./实验一爬取数据\", exist_ok=True)\n",
    "\n",
    "# 循环处理每个年份\n",
    "for year in years:\n",
    "    # 构造年份对应的索引页面URL\n",
    "    index_url = f\"https://www.cnipa.gov.cn/tjxx/jianbao/year{year}/indexy.html\"\n",
    "    \n",
    "    try:\n",
    "        # 发送HTTP请求获取索引页面内容\n",
    "        response = requests.get(index_url, headers=headers)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"获取索引页面 {index_url} 失败: {e}\")\n",
    "        continue\n",
    "\n",
    "    # 解析索引页面，寻找包含特定关键字的链接\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # 筛选包含特定关键字的链接\n",
    "    target_keywords = ['专利申请代理状况', '专利申请、授权按IPC分类分布状况', '商标申请、注册及有效注册状况']\n",
    "    target_link_pages = []\n",
    "    for link in links:\n",
    "        # 检查链接文本是否包含目标关键词之一\n",
    "        if any(keyword in link.text for keyword in target_keywords):\n",
    "            href = link['href']\n",
    "            # 构造完整的链接URL\n",
    "            full_url = urljoin(index_url, href)\n",
    "            target_link_pages.append(full_url)\n",
    "    \n",
    "    # 对每个目标链接页面进行进一步处理\n",
    "    link_dict[year] = []  # 初始化当年份的列表\n",
    "    for target_link_page in target_link_pages:\n",
    "        try:\n",
    "            # 发送HTTP请求获取目标链接页面内容\n",
    "            target_response = requests.get(target_link_page, headers=headers)\n",
    "            target_response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"获取目标链接页面 {target_link_page} 失败: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # 解析目标链接页面，提取其中的所有链接\n",
    "        target_soup = BeautifulSoup(target_response.text, 'html.parser')\n",
    "        sub_links = target_soup.find_all('a', href=True)\n",
    "        \n",
    "        # 过滤并整理子链接，排除包含 'indexy' 的链接\n",
    "        sub_target_links = []\n",
    "        for sub_link in sub_links:\n",
    "            sub_href = sub_link['href']\n",
    "            # 过滤掉非HTML文件的链接以及包含 'indexy' 的链接\n",
    "            if sub_href.endswith('.html') and 'indexy' not in sub_href:\n",
    "                # 构造完整的子链接URL\n",
    "                sub_full_url = urljoin(target_link_page, sub_href)\n",
    "                sub_target_links.append(sub_full_url)\n",
    "        \n",
    "        # 假设目标链接页面的标题作为大类名称\n",
    "        # 尝试从目标链接页面中提取大类名称\n",
    "        class_name = target_soup.find('title')\n",
    "        if class_name:\n",
    "            class_name = class_name.text.strip()\n",
    "        else:\n",
    "            # 如果没有标题，使用链接中的文件名作为大类名称\n",
    "            class_name = os.path.basename(target_link_page).split('.')[0]\n",
    "        \n",
    "        # 将大类名称和子链接数组存储到链接字典中\n",
    "        link_dict[year].append({\n",
    "            'class_name': class_name,\n",
    "            'links': sub_target_links\n",
    "        })\n",
    "    \n",
    "    print(f\"已处理年份 {year} 的链接\")\n",
    "\n",
    "# 保存链接字典到JSON文件\n",
    "with open('./实验一爬取数据/link_dict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(link_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"链接字典已保存到 ./实验一爬取数据/link_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8640149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process 专利申请代理状况-d1: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请代理状况-d2: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请代理状况-d3: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请代理状况-d4: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e3: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e4: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e5: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e6: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e7: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e8: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 商标申请、注册及有效注册状况-h1: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 商标申请、注册及有效注册状况-h2: index 0 is out of bounds for axis 0 with size 0\n",
      "Failed to process 商标申请、注册及有效注册状况-h3: index 0 is out of bounds for axis 0 with size 0\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h4.csv\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h4.csv\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-h4.csv\n",
      "Successfully saved 专利申请代理状况-d1.csv\n",
      "Successfully saved 专利申请代理状况-d2.csv\n",
      "Successfully saved 专利申请代理状况-d3.csv\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Failed to process 专利申请、授权按IPC分类分布状况-e2: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e3.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e4.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e5.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e6.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e7.csv\n",
      "Successfully saved 专利申请、授权按IPC分类分布状况-e8.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g1.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g2.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g3.csv\n",
      "Successfully saved 商标申请、注册及有效注册状况-g4.csv\n",
      "处理失败的链接已保存到 '实验一爬取数据/failed_links.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 设置请求头，模拟浏览器访问\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "failed_links = []\n",
    "\n",
    "def crawl_and_process(year, class_name, link, table_title):\n",
    "    \"\"\"\n",
    "    爬取并处理网页表格数据，并保存为CSV文件\n",
    "    \"\"\"\n",
    "    # 创建对应年份的文件夹（如果不存在的话）\n",
    "    folder_path = f'实验一爬取数据/{year}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # 发送 GET 请求获取网页内容\n",
    "    response = requests.get(link, headers=headers)\n",
    "    # 设置正确的编码格式\n",
    "    response.encoding = 'utf-8'\n",
    "    # 使用 BeautifulSoup 解析 HTML 内容\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # 查找所有表格\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # 检查是否存在表格\n",
    "    if not tables:\n",
    "        print(f\"No tables found in {link}\")\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"No tables found\"})\n",
    "        return\n",
    "    \n",
    "    # 尝试获取第二个表格（根据之前的代码逻辑）\n",
    "    specified_tables = tables[1] if len(tables) > 1 else tables[0]\n",
    "    \n",
    "    # 将表格转换为 DataFrame\n",
    "    try:\n",
    "        df = pd.read_html(str(specified_tables))[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse table in {link}: {e}\")\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": str(e)})\n",
    "        return\n",
    "    \n",
    "    # 对表格进行数据处理\n",
    "    if len(df) > 4:\n",
    "        df = df[4:]\n",
    "        # 检查索引是否存在，避免越界错误\n",
    "        indices_to_drop = [idx for idx in [4, 6] if idx in df.index]\n",
    "        df.drop(indices_to_drop, inplace=True)\n",
    "    \n",
    "    # 检查 DataFrame 是否为空\n",
    "    if df.empty:\n",
    "        print(f'No data to save for {class_name}-{table_title}')\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"Empty DataFrame\"})\n",
    "        return\n",
    "    \n",
    "    # 检查列数是否一致\n",
    "    if len(df.columns) < 1:\n",
    "        print(f'No columns found in DataFrame for {class_name}-{table_title}')\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"No columns found\"})\n",
    "        return\n",
    "    \n",
    "    # 尝试进行更复杂的数据处理\n",
    "    try:\n",
    "        # 设置第一列作为索引\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "        df.index.name = None\n",
    "        \n",
    "        # 将第一行作为列名\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        df.columns.name = None\n",
    "        \n",
    "        # 将原有索引设置为一级索引，同时将第一列和第一行作为二级索引\n",
    "        df = df.set_index(df.columns[0], append=True)\n",
    "        df.index.name = None  # 删除索引列的名称\n",
    "        \n",
    "        # 创建二级索引\n",
    "        df.columns = pd.MultiIndex.from_arrays([df.iloc[0], df.columns], names=[None, None])\n",
    "        df = df.iloc[1:]  # 删除原始的第一行\n",
    "        df = df.swaplevel(axis=1)\n",
    "        \n",
    "        # 保存为 CSV 文件，如果 DataFrame 不为空\n",
    "        if not df.empty:\n",
    "            df.to_csv(f'{folder_path}/{class_name}-{table_title}.csv', encoding='utf-8-sig')\n",
    "            print(f'Successfully saved {class_name}-{table_title}.csv')\n",
    "        else:\n",
    "            print(f'No data to save for {class_name}-{table_title}')\n",
    "            failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": \"Empty DataFrame after processing\"})\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {class_name}-{table_title}: {e}\")\n",
    "        failed_links.append({\"year\": year, \"class_name\": class_name, \"link\": link, \"table_title\": table_title, \"reason\": str(e)})\n",
    "\n",
    "# 加载 JSON 文件\n",
    "with open(r'实验一爬取数据/link_dict.json', 'r', encoding='utf-8') as f:\n",
    "    link_dict = json.load(f)\n",
    "\n",
    "# 遍历每个年份和类别链接，进行爬取和处理\n",
    "for year in link_dict:\n",
    "    if link_dict[year]:  # 如果该年份的类别列表不为空\n",
    "        for class_info in link_dict[year]:\n",
    "            class_name = class_info[\"class_name\"]\n",
    "            for link in class_info[\"links\"]:\n",
    "                # 从链接中提取表格标题\n",
    "                table_title = link.split('/')[-1].replace('.html', '')\n",
    "                crawl_and_process(year, class_name, link, table_title)\n",
    "\n",
    "# 保存处理失败的链接到 JSON 文件\n",
    "with open(r'实验一爬取数据/failed_links.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(failed_links, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"处理失败的链接已保存到 '实验一爬取数据/failed_links.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b502a",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4058621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collected data for 2000\n",
      "Successfully collected data for 2001\n",
      "Successfully collected data for 2002\n",
      "Successfully collected data for 2003\n",
      "Successfully collected data for 2004\n",
      "Successfully collected data for 2005\n",
      "Successfully collected data for 2006\n",
      "Successfully collected data for 2007\n",
      "Successfully collected data for 2008\n",
      "Successfully collected data for 2009\n",
      "Successfully collected data for 2010\n",
      "Successfully collected data for 2011\n",
      "Successfully collected data for 2012\n",
      "Successfully collected data for 2013\n",
      "Successfully collected data for 2014\n",
      "Successfully collected data for 2015\n",
      "Successfully collected data for 2016\n",
      "Successfully collected data for 2017\n",
      "Successfully collected data for 2018\n",
      "Successfully collected data for 2019\n",
      "Successfully collected data for 2020\n",
      "Successfully collected data for 2021\n",
      "Successfully collected data for 2022\n",
      "Successfully collected data for 2023\n",
      "Successfully collected data for 2024\n",
      "Data collection completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "url = 'https://www.baiten.cn/results/filter'\n",
    "\n",
    "# Select Zhejiang province\n",
    "province = '浙江'\n",
    "\n",
    "df = pd.DataFrame(dtype=str)\n",
    "\n",
    "for year in range(2000, 2025):\n",
    "    params = {\n",
    "        'q': 'aa:((' + province + ')) AND (pd:[' + str(year) + '0101 TO ' + str(year) + '1231])',\n",
    "        'fq': '',\n",
    "        'sc': '35184372088831'\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Cookie': '_uab_collina=171636084297048745836638; yunsuo_session_verify=651468e84a027b9680fb898cb3261dac; JSESSIONID=467E8A8E43574AD6ADDD2CA1AA8E598B; zlcp2024423=true; BSESSION=e2148644ecde040ae13f44b2ace4184a38f0d6b0623961ad; PD=a6e2323d0e6adfd16b0bb9a0e28457c5d0c7bf66bf1f72327d2d56e14cb82af5a5f1e3e7ae91cf98c7fa13fa30f3d1fd'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(url=url, params=params, headers=headers)\n",
    "        page_json = resp.json()\n",
    "        \n",
    "        # Extract patent counts by type\n",
    "        cn_um = int(page_json['facetPivots']['country']['cn'].get('cn_um', 0))  # Utility model\n",
    "        cn_id = int(page_json['facetPivots']['country']['cn'].get('cn_id', 0))  # Design\n",
    "        cn_in = int(page_json['facetPivots']['country']['cn'].get('cn_in', 0))  # Invention\n",
    "        cn_gp = int(page_json['facetPivots']['country']['cn'].get('cn_gp', 0))  # Granted invention\n",
    "        cn_in_gp = int(page_json['facetPivots']['country']['cn'].get('cn_in_gp', 0))  # Published invention\n",
    "        \n",
    "        # Add to dataframe\n",
    "        df = pd.concat([df, pd.DataFrame({\n",
    "            '发明专利': cn_in,\n",
    "            '发明公开专利': cn_in_gp,\n",
    "            '发明授权专利': cn_gp,\n",
    "            '外观专利': cn_id,\n",
    "            '实用新型专利': cn_um,\n",
    "            '年份': year,\n",
    "            '省份': province\n",
    "        }, index=[len(df)])], axis=0)\n",
    "        \n",
    "        print(f'Successfully collected data for {year}')\n",
    "        sleep(random.randint(1,3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Failed to collect data for {year}: {e}')\n",
    "        continue\n",
    "\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel(f'./实验一爬取数据/佰腾网/佰腾网{province}2000-2024专利数据.xlsx', index=False)\n",
    "print('Data collection completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5998e9",
   "metadata": {},
   "source": [
    "3（验证码获取次数已超限（8513），爬不了了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26221c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取微博登录cookie...\n",
      "请在浏览器中完成登录...\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: Failed to decode response from marionette\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWebDriverException\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    150\u001b[39m weibo_id = \u001b[33m'\u001b[39m\u001b[33m6188041084\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# 浙江知识产权\u001b[39;00m\n\u001b[32m    151\u001b[39m target_month = \u001b[33m'\u001b[39m\u001b[33m2024-04\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43mcrawl_weibo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweibo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_month\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mcrawl_weibo\u001b[39m\u001b[34m(weibo_id, target_month)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 先获取新的cookie\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m正在获取微博登录cookie...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m cookie = \u001b[43mget_weibo_cookie\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m base_url = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://weibo.cn/u/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweibo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?page=\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     53\u001b[39m headers = {\n\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     55\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mtext/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCookie\u001b[39m\u001b[33m'\u001b[39m: cookie  \u001b[38;5;66;03m# 使用新获取的cookie\u001b[39;00m\n\u001b[32m     60\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mget_weibo_cookie\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m请在浏览器中完成登录...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m登录完成后按回车继续...\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# 添加手动确认步骤\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m cookies = \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cookies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m cookie_str = \u001b[33m'\u001b[39m\u001b[33m; \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcookie[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcookie[\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cookie \u001b[38;5;129;01min\u001b[39;00m cookies])\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cookie_str\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:734\u001b[39m, in \u001b[36mWebDriver.get_cookies\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cookies\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    723\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns a set of dictionaries, corresponding to cookies visible in\u001b[39;00m\n\u001b[32m    724\u001b[39m \u001b[33;03m    the current session.\u001b[39;00m\n\u001b[32m    725\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    732\u001b[39m \u001b[33;03m    >>> cookies = driver.get_cookies()\u001b[39;00m\n\u001b[32m    733\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET_ALL_COOKIES\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:448\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    446\u001b[39m response = \u001b[38;5;28mself\u001b[39m.command_executor.execute(driver_command, params)\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Coding/python-data-analysis/.venv/lib64/python3.13/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mWebDriverException\u001b[39m: Message: Failed to decode response from marionette\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from lxml import etree\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def get_weibo_cookie():\n",
    "    \"\"\"获取微博cookie\"\"\"\n",
    "    options = FirefoxOptions()\n",
    "    # 移除headless模式，显示浏览器窗口\n",
    "    # options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    service = Service(executable_path='/home/gyh/bin/firefoxdriver/geckodriver')\n",
    "    driver = webdriver.Firefox(service=service, options=options)\n",
    "    \n",
    "    try:\n",
    "        # 修改为移动版微博登录页面\n",
    "        driver.get(\"https://weibo.cn/login/\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        print(\"请在浏览器中完成登录...\")\n",
    "        input(\"登录完成后按回车继续...\")  # 添加手动确认步骤\n",
    "        \n",
    "        cookies = driver.get_cookies()\n",
    "        cookie_str = '; '.join([f\"{cookie['name']}={cookie['value']}\" for cookie in cookies])\n",
    "        return cookie_str\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def crawl_weibo(weibo_id, target_month):\n",
    "    \"\"\"抓取指定微博账号的数据\"\"\"\n",
    "    output_dir = './实验一爬取数据/微博数据/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 先获取新的cookie\n",
    "    print(\"正在获取微博登录cookie...\")\n",
    "    cookie = get_weibo_cookie()\n",
    "    \n",
    "    base_url = f'https://weibo.cn/u/{weibo_id}?page='\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cookie': cookie  # 使用新获取的cookie\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    page = 1\n",
    "    max_retries = 3\n",
    "    \n",
    "    try:\n",
    "        while page <= 50:  # 限制最多爬取50页\n",
    "            print(f'正在爬取第{page}页...')\n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "                try:\n",
    "                    response = requests.get(f'{base_url}{page}', headers=headers, timeout=10)\n",
    "                    response.encoding = 'utf-8'\n",
    "                    tree = etree.HTML(response.text)\n",
    "                    \n",
    "                    if '未找到' in response.text or '没有内容' in response.text:\n",
    "                        print('没有更多内容了')\n",
    "                        return\n",
    "                    \n",
    "                    weibo_divs = tree.xpath('//div[@class=\"c\" and @id]')\n",
    "                    if not weibo_divs:\n",
    "                        print('页面未包含微博内容，可能需要重新登录')\n",
    "                        return\n",
    "                    \n",
    "                    for div in weibo_divs:\n",
    "                        try:\n",
    "                            time_str = div.xpath('.//span[@class=\"ct\"]/text()')[0].split('来自')[0].strip()\n",
    "                            post_time = format_time(time_str)\n",
    "                            \n",
    "                            if not post_time.startswith(target_month):\n",
    "                                continue\n",
    "                            content = extract_full_content(div, headers)\n",
    "                            \n",
    "                            try:\n",
    "                                like_count = div.xpath('.//a[contains(text(), \"赞[\")]/text()')[0][2:-1]\n",
    "                                repost_count = div.xpath('.//a[contains(text(), \"转发[\")]/text()')[0][3:-1]\n",
    "                                comment_count = div.xpath('.//a[contains(text(), \"评论[\")]/text()')[0][3:-1]\n",
    "                                comment_link = div.xpath('.//a[contains(text(), \"评论[\")]/@href')[0]\n",
    "                            except IndexError:\n",
    "                                like_count = '0'\n",
    "                                repost_count = '0'\n",
    "                                comment_count = '0'\n",
    "                                comment_link = ''\n",
    "                            \n",
    "                            comments = extract_comments(comment_link, headers) if comment_link else []\n",
    "                            \n",
    "                            data.append({\n",
    "                                '发布时间': post_time,\n",
    "                                '微博内容': content,\n",
    "                                '点赞数': like_count,\n",
    "                                '转发数': repost_count,\n",
    "                                '评论数': comment_count,\n",
    "                                '评论内容': '||'.join(comments)\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"成功获取一条微博：{content[:30]}...\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"处理单条微博时出错: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    break  # 成功获取数据后跳出重试循环\n",
    "                    \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    retries += 1\n",
    "                    print(f\"请求失败，第{retries}次重试: {e}\")\n",
    "                    if retries == max_retries:\n",
    "                        print(f\"第{page}页爬取失败，跳过\")\n",
    "                        break\n",
    "                    time.sleep(random.uniform(5, 10))\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(random.uniform(3, 5))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"爬取过程中出错: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            output_file = os.path.join(output_dir, f'知识产权_{target_month}_微博数据.csv')\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            print(f'数据已保存到: {output_file}')\n",
    "            print(f'共爬取到 {len(data)} 条微博')\n",
    "        else:\n",
    "            print('未获取到任何数据')\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    weibo_id = '6188041084'  # 浙江知识产权\n",
    "    target_month = '2024-04'\n",
    "    \n",
    "    crawl_weibo(weibo_id, target_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d3557",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a3ae679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在抓取新闻列表...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 29 条有效新闻\n",
      "正在处理 1/29: 习近平对精神文明建设工作作出重要指示\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.633 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 2/29: 中高考临近，请收下这份考前调适“锦囊”\n",
      "正在处理 3/29: \n",
      "正在处理 4/29: 202个全国文明城市(区)、60名(组)全国道德模范等受表彰\n",
      "正在处理 5/29: \n",
      "正在处理 6/29: 国家医保局：推动100余项新治疗项目加快进入临床\n",
      "正在处理 7/29: 神二十乘组首次出舱 为何半夜起床关门？\n",
      "正在处理 8/29: \"央企内推\"\"直签保录\"?当心就业不成遭欺诈\n",
      "正在处理 9/29: 警惕身边的“隐形烟害”：二手烟与三手烟的健康威胁\n",
      "正在处理 10/29: 202个全国文明城市（区）、60名（组）全国道德模范等受表彰\n",
      "成功保存 10 条数据到 ./news_data/河北新闻/hebei_news_202505232254.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 全局配置\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://www.hebnews.cn/'\n",
    "}\n",
    "BASE_URL = \"https://world.hebnews.cn/\"  # 国际新闻板块\n",
    "MAX_RETRY = 3\n",
    "REQUEST_TIMEOUT = 15\n",
    "\n",
    "def get_valid_selector(soup):\n",
    "    \"\"\"动态获取有效的选择器\"\"\"\n",
    "    # 尝试多种选择器方案\n",
    "    selectors = [\n",
    "        ('div.news_list > ul > li', 'h3 a', 'div.news_time'),  # 常见结构\n",
    "        ('div.news-list > div.news-item', 'a.title', 'span.time'),  # 备用方案\n",
    "        ('ul.list > li', 'a', 'div.meta')  # 通用方案\n",
    "    ]\n",
    "    \n",
    "    for list_sel, title_sel, time_sel in selectors:\n",
    "        items = soup.select(list_sel)\n",
    "        if len(items) > 3:  # 有效列表判断\n",
    "            return (list_sel, title_sel, time_sel)\n",
    "    return (None, None, None)\n",
    "\n",
    "def parse_news_list(url):\n",
    "    \"\"\"解析新闻列表页\"\"\"\n",
    "    news_items = []\n",
    "    retry = 0\n",
    "    \n",
    "    while retry < MAX_RETRY:\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT, verify=False)\n",
    "            response.encoding = 'utf-8'\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "                list_sel, title_sel, time_sel = get_valid_selector(soup)\n",
    "                \n",
    "                if not list_sel:\n",
    "                    print(\"未找到有效的列表选择器\")\n",
    "                    return []\n",
    "                \n",
    "                for item in soup.select(list_sel):\n",
    "                    try:\n",
    "                        title_elem = item.select_one(title_sel)\n",
    "                        time_elem = item.select_one(time_sel) if time_sel else None\n",
    "                        \n",
    "                        if not title_elem:\n",
    "                            continue\n",
    "                            \n",
    "                        news_url = urljoin(url, title_elem['href'])\n",
    "                        title = title_elem.get_text(strip=True)\n",
    "                        publish_time = time_elem.get_text(strip=True) if time_elem else '未知时间'\n",
    "                        \n",
    "                        # URL格式验证\n",
    "                        if '/content_' in news_url and news_url.endswith('.htm'):\n",
    "                            news_items.append({\n",
    "                                'title': title,\n",
    "                                'time': publish_time,\n",
    "                                'url': news_url\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"解析条目异常: {str(e)[:50]}\")\n",
    "                return news_items\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"请求失败，正在重试({retry+1}/{MAX_RETRY}): {str(e)[:50]}\")\n",
    "            retry += 1\n",
    "            time.sleep(2**retry + random.random())\n",
    "    \n",
    "    print(f\"无法获取列表页: {url}\")\n",
    "    return []\n",
    "\n",
    "def parse_news_detail(url):\n",
    "    \"\"\"解析新闻详情页\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # 提取核心内容\n",
    "        content = soup.select_one('div.article-content') or soup.select_one('div.content')\n",
    "        if content:\n",
    "            # 清理无关元素\n",
    "            for elem in content.select('script, style, div.ads'):\n",
    "                elem.decompose()\n",
    "            content_text = content.get_text('\\n', strip=True)\n",
    "        else:\n",
    "            content_text = \"内容解析失败\"\n",
    "            \n",
    "        # 提取来源\n",
    "        source = soup.find('span', class_='source') or soup.find('div', class_='info')\n",
    "        source = source.get_text(strip=True) if source else \"未知来源\"\n",
    "        \n",
    "        return {\n",
    "            'content': content_text,\n",
    "            'source': source,\n",
    "            'keywords': extract_keywords(content_text)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"详情页解析失败: {str(e)[:50]}\")\n",
    "        return None\n",
    "\n",
    "def extract_keywords(text, top_n=5):\n",
    "    \"\"\"简单关键词提取（示例）\"\"\"\n",
    "    from collections import Counter\n",
    "    words = [word for word in jieba.cut(text) if len(word) >= 2]\n",
    "    return ', '.join([w for w, _ in Counter(words).most_common(top_n)])\n",
    "\n",
    "def main():\n",
    "    # 配置输出\n",
    "    output_dir = './news_data/河北新闻/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_news = []\n",
    "    \n",
    "    # 获取列表页数据\n",
    "    print(\"正在抓取新闻列表...\")\n",
    "    news_list = parse_news_list(BASE_URL)\n",
    "    print(f\"发现 {len(news_list)} 条有效新闻\")\n",
    "    \n",
    "    # 处理详情页\n",
    "    for idx, news in enumerate(news_list[:10]):  # 测试时限制数量\n",
    "        print(f\"正在处理 {idx+1}/{len(news_list)}: {news['title']}\")\n",
    "        detail = parse_news_detail(news['url'])\n",
    "        if detail:\n",
    "            all_news.append({**news, **detail})\n",
    "        \n",
    "        # 动态延迟（根据服务器响应调整）\n",
    "        time.sleep(random.uniform(1.5, 3.5))\n",
    "    \n",
    "    # 保存结果\n",
    "    if all_news:\n",
    "        df = pd.DataFrame(all_news)\n",
    "        df = df[['title', 'time', 'source', 'keywords', 'content', 'url']]\n",
    "        output_path = os.path.join(output_dir, f'hebei_news_{time.strftime(\"%Y%m%d%H%M\")}.csv')\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"成功保存 {len(df)} 条数据到 {output_path}\")\n",
    "    else:\n",
    "        print(\"未获取到有效数据\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 安装中文分词库（可选）\n",
    "    try:\n",
    "        import jieba\n",
    "    except ImportError:\n",
    "        print(\"如需使用关键词提取功能，请先安装jieba：pip install jieba\")\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
