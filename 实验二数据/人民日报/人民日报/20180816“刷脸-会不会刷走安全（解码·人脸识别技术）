
　　随着数据的积累、计算机算力的跃升和算法的优化，人工智能正在让生活变得高效。语音识别、图像识别使身份认证更可信赖，短短几秒就能证明“你就是你”；智能诊疗和自动驾驶，更让人们看到了战胜疾病、减少事故的新机会；人工智能还可以轻松战胜围棋高手，写出优美的诗句……其自主性和创造性正在模糊人和机器的分野。　　但是，当隐私侵犯、数据泄露、算法偏见等事件层出不穷时，人们又不得不反思：人工智能的持续进步和广泛应用带来的好处是巨大的，为了让它真正有益于社会，同样不能忽视的还有对人工智能的价值引导、伦理调节以及风险规制。　　“刷脸”应用更广泛，对隐私权的威胁值得重视　　“刷脸”进站、“刷脸”支付、“刷脸”签到、“刷脸”执法……人脸识别技术正走进更为广阔的应用场景，与指纹、虹膜等相比，人脸是一个具有弱隐私性的生物特征，因此，这一技术对于公民隐私保护造成的威胁性尤其值得重视。“人脸图像或视频广义上讲也是数据，如果没有妥善保管和合理使用，就会容易侵犯用户的隐私。”中国社会科学院哲学研究所研究员段伟文说。　　通过数据采集和机器学习来对用户的特征、偏好等“画像”，互联网服务商进而提供一些个性化的服务和推荐等，从正面看是有利于供需双方的一种互动。但对于消费者来说，这种交换是不对等的。就频频发生的个人数据侵权的事件来看，个人数据权利与机构数据权力的对比已经失衡，在对数据的收集和使用方面，消费者是被动的，企业和机构是主动的。段伟文表示，“数据实际上成为被企业垄断的资源，又是驱动经济的要素。”如果商家只从自身利益出发，就难免会对个人数据过度使用或者不恰当披露。　　“大数据时代，个人在互联网上的任何行为都会变成数据被沉淀下来，而这些数据的汇集都可能最终导致个人隐私的泄露。”湖南师范大学人工智能道德决策研究所所长李伦认为，用户已经成为被观察、分析和监测的对象。　　算法应更客观透明，要避免歧视与“杀熟”　　在信息爆炸的时代，数据的处理、分析、应用很多都是由算法来实现的，越来越多的决策正被算法所取代。从内容推荐到广告投放，从信用额度评估到犯罪风险评估，算法无处不在——它操作的自动驾驶或许比司机更加安全，它得出的诊断结果可能比医生更准确，越来越多的人开始习惯一个由算法构建的“打分”社会。　　作为一种信息技术，算法在拨开信息和数据“迷雾”的同时，也面临着伦理上的挑战：利用人工智能来评估犯罪风险，算法可以影响刑罚；当自动驾驶汽车面临危险，算法可以决定牺牲哪一方；应用于武器系统的算法甚至可以决定攻击的目标……由此引发了一个不容忽视的问题：如何确保算法的公正？　　腾讯研究院法律研究中心高级研究员曹建峰认为，即使作为一种数学表达，算法本质上也是“以数学方式或者计算机代码表达的意见”。算法的设计、模型、目的、成功标准、数据使用等，都是编程人员的主观选择，偏见会有意或者无意地嵌入算法，使之代码化。“算法并不客观，在算法决策起作用的诸多领域，算法歧视也并不鲜见。”　　“算法决策多数情况下是一种预测，用过去的数据预测未来的趋势，算法模型和数据输入决定着预测的结果，因此这两个要素也就成为算法歧视的主要来源。”曹建峰解释说，除了主观因素以外，数据本身也会影响算法的决策和预测。“数据是社会现实的反映，数据可能是不正确、不完整或者过时的，训练数据本身也可能是歧视性的，用这样的数据训练出来的算法系统，自然也会带上歧视的烙印。”　　2016年3月，微软人工智能聊天机器人Tay上线，在与网民互动过程中，很短时间内就“误入歧途”，集性别歧视、种族歧视于一身，最终微软不得不让它“下岗”。曹建峰认为，算法倾向于将歧视固化或放大，使歧视长存于整个算法之中。因此，如果将算法应用在犯罪评估、信用贷款、雇佣评估等关系人们切身利益的场合，一旦产生歧视，就可能危害个人乃至社会的利益。　　此外，深度学习还是一个典型的“黑箱”算法，可能连设计者都不知道算法如何决策，因而要在系统中发现是否存在歧视和歧视根源，技术上也较为困难。“算法的‘黑箱’特征使其决策逻辑缺乏透明性和可解释性。”李伦说，随着大数据“杀熟”、算法歧视等事件的出现，社会对算法的质疑也逐渐增多。政府和企业在使用数据的过程中，必须提高对公众的透明度，让选择权回归个人。　　加强核查监管，加大对数据滥用等行为的惩戒力度　　2017年7月，国务院印发《新一代人工智能发展规划》（以下简称《规划》）。《规划》强调，促进人工智能行业和企业自律，切实加强管理，加大对数据滥用、侵犯个人隐私、违背道德伦理等行为的惩戒力度。　　“虽然‘刷脸’的应用越来越多，但人工智能目前仍处于起步阶段，需加大对数据和隐私的保护力度，关注和防范由算法滥用所导致的决策失误和社会不公。”在个人数据权利的保护方面，段伟文建议，应促使数据交易各方对自己的行为负责，让每个人知道自己的数据如何被处理，特别是用于其他用途的情形，减少数据滥用，让人们清楚知道自己的“脸”还是否安全。　　段伟文认为，要进一步加强人工智能的伦理设计，对算法的理论预设、内在机制与实践语境等进行全流程追问与核查，从算法决策的结果和影响中的不公正入手，反向核查其机制与过程有无故意或不自觉的曲解与误导，揭示存在的问题，并促使其修正和改进。　　在曹建峰看来，应对人工智能带来的伦理问题，一是要构建算法治理的内外部约束机制，将人类社会的法律、道德等规范和价值嵌入人工智能系统；二是在人工智能研发中贯彻伦理原则，促使研发人员遵守基本的伦理准则；三是对算法进行必要的监管，提升算法自身的代码透明性和算法决策的透明性；四是针对算法决策和歧视以及造成的人身财产损害，提供法律救济。　　“我们生活在一个人机共生的时代，人类与机器之间势必将发生各种冲突和矛盾，仅靠法律和制度很难完全解决。”李伦表示，人们还应努力提升自身的科学素养，主动维护自身的权利，社会也应尽快建立讨论人工智能伦理问题的公共平台，让各方充分表达意见，促进共识的形成。

